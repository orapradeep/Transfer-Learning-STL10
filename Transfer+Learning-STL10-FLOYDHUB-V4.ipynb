{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=5, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Image Classification\n",
    "# Author : Pradeep S Naulia\n",
    "# =========================\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout, Activation\n",
    "from keras.layers import MaxPooling2D, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "\n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib  # ugly but works\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Image shape, EPOCHs, directories to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image shape\n",
    "HEIGHT = 96\n",
    "WIDTH = 96\n",
    "DEPTH = 3\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "# size of a single image in bytes\n",
    "SIZE = HEIGHT * WIDTH * DEPTH\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# url of the binary data\n",
    "DATA_URL = 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz'\n",
    "\n",
    "# path to the binary train file with image data\n",
    "DATA_PATH = './data/stl10_binary/train_X.bin'\n",
    "\n",
    "# path to the binary train file with labels\n",
    "LABEL_PATH = './data/stl10_binary/train_y.bin'\n",
    "\n",
    "\n",
    "# path to the binary test file with image data\n",
    "TEST_DATA_PATH = './data/stl10_binary/test_X.bin'\n",
    "\n",
    "# path to the binary test file with labels\n",
    "TEST_LABEL_PATH = './data/stl10_binary/test_y.bin'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to do the following:\n",
    "1. convert the data format from binary to vector format\n",
    "2. Also download and store the data if not downloaded already\n",
    "3. Also define a Model() instance in Keras using CNN in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "\n",
    "\n",
    "def read_single_image(image_file):\n",
    "    \"\"\"\n",
    "    CAREFUL! - this method uses a file as input instead of the path - so the\n",
    "    position of the reader will be remembered outside of context of this method.\n",
    "    :param image_file: the open file containing the images\n",
    "    :return: a single image\n",
    "    \"\"\"\n",
    "    # read a single image, count determines the number of uint8's to read\n",
    "    image = np.fromfile(image_file, dtype=np.uint8, count=SIZE)\n",
    "    # force into image matrix\n",
    "    image = np.reshape(image, (3, 96, 96))\n",
    "    # transpose to standard format\n",
    "    # You might want to comment this line or reverse the shuffle\n",
    "    # if you will use a learning algorithm like CNN, since they like\n",
    "    # their channels separated.\n",
    "    image = np.transpose(image, (2, 1, 0))\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_image(image):\n",
    "    \"\"\"\n",
    "    :param image: the image to be plotted in a 3-D matrix format\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def download_and_extract():\n",
    "    \"\"\"\n",
    "    Download and extract the STL-10 dataset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.urlretrieve(\n",
    "            DATA_URL, filepath, reporthook=_progress)\n",
    "        print('Downloaded', filename)\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "\n",
    "\n",
    "def MyClassifierModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the MyClassifierModel.\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"input_shape : \", input_shape)\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    #======= Start 1 ===========\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (3, 3), strides=(1, 1), name='conv0')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    print(\"Conv 1 : \", X.shape)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    print(\"MaxPooling 1 : \", X.shape)\n",
    "\n",
    "    #======= End 1 ===========\n",
    "\n",
    "    #======= Start 2 ===========\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(128, (3, 3), strides=(1, 1), name='conv1')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn1')(X)\n",
    "    # X = Dropout(0.2)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    print(\"Conv 2 : \", X.shape)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool1')(X)\n",
    "\n",
    "    print(\"MaxPooling 2 : \", X.shape)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    #======== End 2 ============\n",
    "\n",
    "    #======= Start 3 ===========\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(256, (3, 3), strides=(1, 1), name='conv2')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn2')(X)\n",
    "    # X = Dropout(0.2)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "    print(\"Conv 3 : \", X.shape)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool2')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    print(\"MaxPooling 3 : \", X.shape)\n",
    "\n",
    "    #======== End 3 ============\n",
    "\n",
    "    #======= Start 4 ===========\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(512, (3, 3), strides=(1, 1), name='conv3')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn3')(X)\n",
    "    # X = Dropout(0.3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    print(\"Conv 4 : \", X.shape)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool3')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    print(\"MaxPooling 4 : \", X.shape)\n",
    "\n",
    "    #======== End 4 ============\n",
    "\n",
    "    #======= Start 5 ===========\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(1024, (3, 3), strides=(1, 1), name='conv4')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn4')(X)\n",
    "    # X = Dropout(0.3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "    print(\"Conv 5 : \", X.shape)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool4')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "\n",
    "    print(\"MaxPooling 5 : \", X.shape)\n",
    "\n",
    "    #======== End 5 ============\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    #X = Dense(512, activation='relu')(X)\n",
    "    #X = Dropout(0.2)(X)\n",
    "    # X = Dense(256, activation='relu')(X)\n",
    "    #X = Dropout(0.2)(X)\n",
    "    X = Dense(10, activation='softmax', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs=X_input, outputs=X, name='MyClassifierModel')\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and fit it\n",
    "Run EPOCHs to build model. Once model is trained we run it on test data to predict the category using softmax probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3)\n",
      "(5000,)\n",
      "(8000, 96, 96, 3)\n",
      "(8000,)\n",
      "input_shape :  (96, 96, 3)\n",
      "Conv 1 :  (?, 100, 100, 32)\n",
      "MaxPooling 1 :  (?, 50, 50, 32)\n",
      "Conv 2 :  (?, 48, 48, 128)\n",
      "MaxPooling 2 :  (?, 24, 24, 128)\n",
      "Conv 3 :  (?, 22, 22, 256)\n",
      "MaxPooling 3 :  (?, 11, 11, 256)\n",
      "Conv 4 :  (?, 9, 9, 512)\n",
      "MaxPooling 4 :  (?, 4, 4, 512)\n",
      "Conv 5 :  (?, 2, 2, 1024)\n",
      "MaxPooling 5 :  (?, 1, 1, 1024)\n",
      "Epoch 1/30\n",
      "5000/5000 [==============================] - 19s - loss: 1.9238 - acc: 0.3502    \n",
      "Epoch 2/30\n",
      "5000/5000 [==============================] - 17s - loss: 1.4424 - acc: 0.4802    \n",
      "Epoch 3/30\n",
      "5000/5000 [==============================] - 17s - loss: 1.2145 - acc: 0.5704    \n",
      "Epoch 4/30\n",
      "5000/5000 [==============================] - 17s - loss: 1.0634 - acc: 0.6248    \n",
      "Epoch 5/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.8841 - acc: 0.6796    \n",
      "Epoch 6/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.7949 - acc: 0.7088    \n",
      "Epoch 7/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.6610 - acc: 0.7628    \n",
      "Epoch 8/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.5891 - acc: 0.7884    \n",
      "Epoch 9/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.5014 - acc: 0.8204    \n",
      "Epoch 10/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.4348 - acc: 0.8370    \n",
      "Epoch 11/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.3893 - acc: 0.8610    \n",
      "Epoch 12/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.2887 - acc: 0.9002    \n",
      "Epoch 13/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.2596 - acc: 0.9084    \n",
      "Epoch 14/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.2469 - acc: 0.9170    \n",
      "Epoch 15/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.1688 - acc: 0.9434    \n",
      "Epoch 16/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.1339 - acc: 0.9560    \n",
      "Epoch 17/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.2032 - acc: 0.9368    \n",
      "Epoch 18/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.1169 - acc: 0.9624    \n",
      "Epoch 19/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.1523 - acc: 0.9486    \n",
      "Epoch 20/30\n",
      "5000/5000 [==============================] - 18s - loss: 0.1384 - acc: 0.9564    \n",
      "Epoch 21/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0771 - acc: 0.9778    \n",
      "Epoch 22/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0595 - acc: 0.9842    \n",
      "Epoch 23/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0562 - acc: 0.9844    \n",
      "Epoch 24/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0479 - acc: 0.9870    \n",
      "Epoch 25/30\n",
      "5000/5000 [==============================] - 18s - loss: 0.0654 - acc: 0.9796    \n",
      "Epoch 26/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.1017 - acc: 0.9674    \n",
      "Epoch 27/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0975 - acc: 0.9690    \n",
      "Epoch 28/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0779 - acc: 0.9756    \n",
      "Epoch 29/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0849 - acc: 0.9732    \n",
      "Epoch 30/30\n",
      "5000/5000 [==============================] - 17s - loss: 0.0963 - acc: 0.9668    \n",
      "7968/8000 [============================>.] - ETA: 0sLoss = 1.61617149591\n",
      "Test Accuracy = 0.6395\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # download data if needed\n",
    "    #download_and_extract()\n",
    "\n",
    "    # test to check if the image is read correctly\n",
    "    '''\n",
    "    with open(DATA_PATH) as f:\n",
    "        image = read_single_image(f)\n",
    "        plot_image(image)\n",
    "        '''\n",
    "\n",
    "    # test to check if the whole dataset is read correctly\n",
    "    train_images = read_all_images(DATA_PATH)\n",
    "    print(train_images.shape)  # (5000, 96, 96, 3)\n",
    "\n",
    "    train_labels = read_labels(LABEL_PATH)\n",
    "    print(train_labels.shape)  # (5000,)\n",
    "    # print(labels)\n",
    "\n",
    "    # test to check if the whole dataset is read correctly\n",
    "    test_images = read_all_images(TEST_DATA_PATH)\n",
    "    print(test_images.shape)  # (8000, 96, 96, 3)\n",
    "\n",
    "    test_labels = read_labels(TEST_LABEL_PATH)\n",
    "    print(test_labels.shape)  # (8000,)\n",
    "    # print(labels)\n",
    "\n",
    "    #---------- Code start ----------------\n",
    "\n",
    "    train_labels = train_labels.reshape(len(train_labels),  1)\n",
    "    test_labels = test_labels.reshape(len(test_labels),  1)\n",
    "\n",
    "    train_labels = train_labels - 1\n",
    "    test_labels = test_labels - 1\n",
    "\n",
    "    train_labels = np_utils.to_categorical(train_labels, num_classes=10)\n",
    "    test_labels = np_utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "    myClassifierModel = MyClassifierModel(train_images.shape[1:])\n",
    "    myClassifierModel.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    myClassifierModel.fit(x=train_images, y=train_labels,\n",
    "                          epochs=EPOCHS, batch_size=64)\n",
    "    preds = myClassifierModel.evaluate(x=test_images, y=test_labels)\n",
    "    print (\"Loss = \" + str(preds[0]))\n",
    "    print (\"Test Accuracy = \" + str(preds[1]))\n",
    "\n",
    "    #---------- Code End ----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy is good but I am working still working on improving it with optimizing other hyperparameters. I tried with regularization and dropouts but it didnt improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
